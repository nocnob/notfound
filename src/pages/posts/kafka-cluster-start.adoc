= kafka 基本使用 (KRaft，集群)
notfound <notfound@notfound.cn>
1.0, 2023-01-02: init

:page-slug: kafka-cluster-start
:page-category: kafka
:page-draft: false

== 准备

- Ubuntu 22.04
- Docker 20.10
- Kafka 3.3.1

[source,bash]
----
docker pull openjdk:17
# 为使用容器静态 IP，创建新的网络
docker network create --driver=bridge --gateway=172.18.1.254 --subnet=172.18.1.0/24 kafka

tar -zxvf kafka_2.13-3.3.1.tgz
----

容器中测试，将 kafka 目录挂载到容器内。节点信息：

|===
| name |IP |hostname | volume

| kafka01 | 172.18.1.1 | kafka01 | /opt/kafka01
| kafka02 | 172.18.1.2 | kafka02 | /opt/kafka02
| kafka03 | 172.18.1.3 | kafka03 | /opt/kafka03
|===


== 配置节点

需要在 3 个节点上执行 3 次。以节点 kafka01 为例：

[source,bash]
----
# 复制解压后的文件
sudo cp -r kafka_2.13-3.3.1 /opt/kafka01
# 修改文件权限
sudo chown -R $(id -u):$(id -g) /opt/kafka01
----

配置说明：

.config/kraft/server.properties
[source,properties]
----
# 服务的承担的角色
process.roles=broker,controller
# 节点 ID ,3 个节点 ID 分别设置为 1、2、 3
node.id=1
# 负责仲裁的控制器，格式为 id1@host1:port1,id2@host2:port2,id3@host3:port3
controller.quorum.voters=1@kafka01:9093,2@kafka02:9093,3@kafka03:9093
# 监听点，3 个节点分别为 kafka01、kafka02、kafka03
advertised.listeners=PLAINTEXT://kafka01:9092
# 日志目录
log.dirs=/opt/kafka/kraft-combined-logs
# 分区数量
num.partitions=3
# 内置偏移量主题 __consumer_offsets 复制系数
offsets.topic.replication.factor=3
# 事务日志复制系数
transaction.state.log.replication.factor=3
# 事务日志最小同步副本数 (in-sync replicas)
transaction.state.log.min.isr=2
----

修改配置：

[source,bash]
----
# 备份
cp config/kraft/server.properties config/kraft/server.properties.bak
# 修改配置
sed -e "s/^node.id=.*/node.id=1/" \
    -e "s/^controller.quorum.voters=.*/controller.quorum.voters=1@kafka01:9093,2@kafka02:9093,3@kafka03:9093/" \
    -e "s/localhost/kafka01/" \
    -e "s/^log.dirs=.*/log.dirs=\/opt\/kafka\/kraft-combined-logs/" \
    -e "s/^offsets.topic.replication.factor=.*/offsets.topic.replication.factor=3/" \
    -e "s/^transaction.state.log.replication.factor=.*/transaction.state.log.replication.factor=3/" \
    -e "s/^transaction.state.log.min.isr=.*/transaction.state.log.min.isr=2/" \
    config/kraft/server.properties.bak | tee config/kraft/server.properties
# 通过 diff 检查变更
diff -u config/kraft/server.properties.bak config/kraft/server.properties
# 或者通过 vimdiff 检查变更
vimdiff config/kraft/server.properties.bak config/kraft/server.properties
----


在容器内初始化 kafka 并启动服务：

[source,bash]
----
# 创建容器
docker run \
   --user=$(id -u) \
   --rm \
   --interactive \
   --tty \
   --network=kafka \
   --ip=172.18.1.1 \
   --name=kafka01 \
   --hostname=kafka01 \
   --volume /opt/kafka01:/opt/kafka \
    openjdk:17 bash

# 生成集群 ID，3 个节点使用相同的 ID
cd /opt/kafka
bin/kafka-storage.sh random-uuid
# 输出：8Jqq_Wm6SGGwuTLGTcu5NA

# 使用集群 ID 格式化日志目录，3 个节点使用相同 ID
export KAFKA_CLUSTER_ID=8Jqq_Wm6SGGwuTLGTcu5NA
bin/kafka-storage.sh format -t $KAFKA_CLUSTER_ID -c config/kraft/server.properties

# 三个节点都初始化完成后，依次启动 kafka 服务
bin/kafka-server-start.sh -daemon config/kraft/server.properties
----

== 测试

进入任意容器内。

=== 负载均衡

同一个主题的不同分区在不同节点，可以进行跨服务器的负载均衡。

[source,bash]
----
export broker=localhost:9092
export topic=foo

# 创建主题，3 个分区， 保存 1 份
bin/kafka-topics.sh --bootstrap-server $broker --create --topic $topic --partitions 3 --replication-factor 1
# 查看主题详情
bin/kafka-topics.sh --bootstrap-server $broker --describe --topic $topic
# 删除主题
bin/kafka-topics.sh --bootstrap-server $broker --delete --topic $topic
----

主题详情：

[source,text]
----
Topic: foo	TopicId: SatE7dwySoWwTDs1JK70kQ	PartitionCount: 3	ReplicationFactor: 1	Configs: segment.bytes=1073741824
	Topic: foo	Partition: 0	Leader: 3	Replicas: 3	Isr: 3
	Topic: foo	Partition: 1	Leader: 1	Replicas: 1	Isr: 1
	Topic: foo	Partition: 2	Leader: 2	Replicas: 2	Isr: 2
----

.主题详情示意图
[source,plantuml]
----
@startuml
node "broker 3" {
  card "Topic foo" as t3 {
    card "Partition 0"
  }
}

node "broker 1" {
  card "Topic foo" as t1 {
    card "Partition 1"
  }
}

node "broker 2" {
  card "Topic foo" as t2 {
    card "Partition 2"
  }
}
@enduml
----
* 主题 foo 上的 3 个分区分布在 3 个节点上

=== 复制

通过复制可避免因为单点故障造成数据丢失。

[source,bash]
----
export broker=localhost:9092
export topic=bar

# 创建主题，1 个分区， 保存 3 份
bin/kafka-topics.sh --bootstrap-server $broker --create --topic $topic --partitions 1 --replication-factor 3 
# 查看主题详情
bin/kafka-topics.sh --bootstrap-server $broker --describe --topic $topic
----

主题详情：

[source,text]
----
Topic: bar	TopicId: sgJStn8BSICA_z-mA5i5mQ	PartitionCount: 1	ReplicationFactor: 3	Configs: segment.bytes=1073741824
	Topic: bar	Partition: 0	Leader: 2	Replicas: 2,3,1	Isr: 2,3,1
----

.主题详情示意图
[source,plantuml]
----
@startuml
node "broker 1" {
  card "Topic bar" as t1 {
    card "Partition 0" as p1
  }
}

node "broker 2(Leader)" {
  card "Topic bar" as t2 {
    card "Partition 0" as p2
  }
}

node "broker 3" {
  card "Topic bar" as t3 {
    card "Partition 0" as p3
  }
}
@enduml
----
* 主题 bar 分区 0 在 3 个节点上各保存了 1 份，其中首领副本为 2

=== 检查偏移主题

进行生产、消费后，会创建内置偏移主题 `__consumer_offsets`

[source,bash]
----
export broker=localhost:9092
export topic=__consumer_offsets

bin/kafka-topics.sh --bootstrap-server $broker --describe --topic $topic
----
* 确定主题开启了多个副本。

== 问题

1. 创建容器时出现 `WARNING: IPv4 forwarding is disabled. Networking will not work.`
+
重启容器：
+
[source,bash]
----
sudo systemctl restart docker.service
----
+
2. 客户端出现 GROUP_COORDINATOR_NOT_AVAILABLE
+
[quote,kafka-go]
____
Group Coordinator Not Available: the broker returns this error code for group coordinator requests, offset commits, and most group management requests if the offsets topic has not yet been created, or if the group coordinator is not active
____
+
主题 `__consumer_offsets` 仅复制 1 份且保存数据的节点故障，无法获取数据偏移信息：
+
[source,bash]
----
export broker=localhost:9092
export topic=__consumer_offsets

# 无法查看到相关消费者群组
bin/kafka-consumer-groups.sh --bootstrap-server $broker --list

# 内置主题 __consumer_offsets 存在不可用分区
bin/kafka-topics.sh --bootstrap-server $broker --describe --topic $topic --unavailable-partitions
# 输出：
# 	Topic: __consumer_offsets	Partition: 2	Leader: none	Replicas: 2	Isr: 2
#   ....
----
+
解决方案，调整内置主题复制系数，确保单个节点故障时偏移量等信息不丢失：
+
[source,properties]
----
# 内置偏移量主题 __consumer_offsets 复制系数
offsets.topic.replication.factor=3
# 事务日志复制系数
transaction.state.log.replication.factor=3
# 事务日志最小同步副本数 (in-sync replicas)
transaction.state.log.min.isr=2
----

== 参考

* https://giraffetree.me/2020/09/03/The-coordinator-is-not-available/
* https://kafka.apache.org/documentation
